{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432d503e",
   "metadata": {},
   "source": [
    "# OpenAI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3a271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7aec0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# to use non-openai models, e.g., from Hugging Face\n",
    "!pip install -Uq \"openai-agents[litellm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc0c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable async in notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526c20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# set default model for agents\n",
    "os.environ[\"OPENAI_DEFAULT_MODEL\"] = \"gpt-5-mini\"\n",
    "\n",
    "# openai API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20992c80",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318e856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear sun warms the streets,  \n",
      "Blue sky over bustling blocks —  \n",
      "New York basks in light.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, function_tool, Runner\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"returns weather info for the specified city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Haiku agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"What's the weather in New York?\")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1516495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(result))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result.__dict__, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcf5d8",
   "metadata": {},
   "source": [
    "## Non Open-AI models\n",
    "\n",
    "To use Hugging Face models with OpenAI Agents SDK, you need to set the `HF_TOKEN` env variable.\n",
    "\n",
    "Then set the model param to:\n",
    "\n",
    "```\n",
    "litellm/huggingface/<provider>/<hf_org_or_user>/<hf_model>\n",
    "```\n",
    "\n",
    "If you prefer to have more control, you can use the `LitellmModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e4f0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cef7e8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='\\n\\n', r...nside the XML tags.\\n'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='to...s={'stop_reason': None}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='\\n\\nNew ...e weather response.\\n'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...s={'stop_reason': None}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "# using kimi-k2-thinking\n",
    "\n",
    "from agents import Agent, Runner, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "hf_model = LitellmModel(\n",
    "    model=\"huggingface/nscale/Qwen/Qwen3-8B\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "hf_agent = Agent(\n",
    "    name=\"Kimi agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    tools=[get_weather],\n",
    "    # model=\"litellm/huggingface/nscale/Qwen/Qwen3-8B\",\n",
    "    model=hf_model,\n",
    "    \n",
    "    # optional, for usage tracking (requires openai API key)\n",
    "    model_settings=ModelSettings(include_usage=True,),\n",
    ")\n",
    "\n",
    "result = await Runner.run(hf_agent, \"What's the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7916a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "New York shines today  \n",
      "Sky stretches wide, bright and clear  \n",
      "Breeze carries cheer\n"
     ]
    }
   ],
   "source": [
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(result.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8393c03",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f624ecb",
   "metadata": {},
   "source": [
    "### Tracing\n",
    "\n",
    "You can trace the execution of the agent on the OpenAI platform. To do this, you need to set an OpenAI API key like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce534e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from agents import set_tracing_export_api_key, Agent, Runner\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "tracing_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "set_tracing_export_api_key(tracing_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fef76",
   "metadata": {},
   "source": [
    "Or you can pass the API key as a parameter to the Runner. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a04987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner, RunConfig\n",
    "\n",
    "await Runner.run(\n",
    "    agent,\n",
    "    input=\"Hello\",\n",
    "    run_config=RunConfig(tracing={\"api_key\": \"sk-...\"}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccb06d",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "- Use the `output_type` param with Pydantic objects (or dataclasses, lists, TypedDicts, etc.)\n",
    "- When using non-openai models, remember to check that they support both Structured Ouput AND tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f84cc7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Appointment with Dr. Drake Ramoray' date='Wednesday' participants=['Dr. Drake Ramoray', 'John']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='{\\n  \"na...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "import os\n",
    "\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_TOKEN\"] = input(\"Enter your Hugging Face token: \")\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "model = LitellmModel(\n",
    "    model=\"huggingface/novita/moonshotai/Kimi-K2-Instruct-0905\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Calendar extractor\",\n",
    "    instructions=\"Extract calendar events from the text\",\n",
    "    output_type=CalendarEvent,\n",
    "    model=model,\n",
    "    # model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "email = \"\"\"\n",
    "Hello, \n",
    "\n",
    "I just wanted to confirm the appointment with Dr. Drake Ramoray for Wednesday at 5 p.m. \n",
    "\n",
    "Please let me know if there is any additional information that you need. \n",
    "\n",
    "Best, John \n",
    "\"\"\"\n",
    "\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    f\"Extract the event from the following text: {email}\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846818ce",
   "metadata": {},
   "source": [
    "### Multi-Agent systems\n",
    "\n",
    "Two main architectures:\n",
    "- Manager (agents as tools): A central manager/orchestrator invokes specialized sub‑agents as tools and retains control of the conversation.\n",
    "- Handoffs: Peer agents hand off control to a specialized agent that takes over the conversation. This is decentralized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0549c",
   "metadata": {},
   "source": [
    "#### Agents Handoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4055da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=\"I'll tra... History Tutor agent.'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='to...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The caus...ists for readability.\"}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causes of World War II are complex and rooted in the unresolved issues of World War I, the rise of totalitarian regimes, and the failure of international diplomacy. Here is a clear breakdown of the key factors that led to the outbreak of the war in 1939.\n",
      "\n",
      "### 1. The Treaty of Versailles (1919)\n",
      "After World War I, the Treaty of Versailles imposed harsh penalties on Germany:\n",
      "*   **War Guilt Clause:** Germany was forced to accept full responsibility for the war.\n",
      "*   **Reparations:** Germany had to pay massive financial reparations to the Allies, which crippled its economy.\n",
      "*   **Territorial Losses:** Germany lost significant territory (e.g., Alsace-Lorraine to France, the Polish Corridor) and all its overseas colonies.\n",
      "*   **Military Restrictions:** The German military was severely limited in size and capability.\n",
      "\n",
      "These measures created deep resentment and humiliation in Germany, creating a fertile ground for radical political movements.\n",
      "\n",
      "### 2. The Rise of Fascism and Totalitarianism\n",
      "During the economic turmoil of the 1920s and 1930s, aggressive dictatorships rose to power, promising national glory and economic recovery:\n",
      "*   **Germany (Nazism):** Adolf Hitler and the Nazi Party rose to power in 1933. Hitler aimed to overturn the Treaty of Versailles, rebuild the German military, and unite all German-speaking people. He also promoted the racist ideology of *Lebensraum* (living space), advocating for the expansion of Germany into Eastern Europe.\n",
      "*   **Italy (Fascism):** Benito Mussolini established a fascist regime in the 1920s with a desire to revive the Roman Empire through territorial conquest (e.g., Ethiopia in 1935).\n",
      "*   **Japan (Militarism):** Japan, lacking natural resources, fell under the control of military leaders who sought to dominate Asia and the Pacific (e.g., the invasion of Manchuria in 1931 and China in 1937).\n",
      "\n",
      "### 3. Weakness of the League of Nations\n",
      "The League of Nations, established after WWI to prevent future conflicts, was ineffective:\n",
      "*   **Lack of Enforcement:** It had no standing army and relied on member nations to enforce sanctions.\n",
      "*   **Absent Powers:** The United States never joined, and the League later failed to stop major acts of aggression by Japan in Manchuria and Italy in Ethiopia. This showed the world that aggression could go unpunished.\n",
      "\n",
      "### 4. Policy of Appeasement\n",
      "In the 1930s, Britain and France, haunted by the memories of WWI trench warfare, were desperate to avoid another conflict. They followed a policy of **appeasement**:\n",
      "*   They allowed Hitler to break the Treaty of Versailles and take territory, hoping this would satisfy him and prevent war.\n",
      "*   The prime example was the **Munich Agreement (1938)**, where Britain and France allowed Germany to annex the Sudetenland region of Czechoslovakia. Churchill famously called this \"a total and unmitigated defeat.\"\n",
      "\n",
      "### 5. Failure of Collective Security and the Nazi-Soviet Pact\n",
      "Ideally, the democratic nations of Europe would form alliances to stop Hitler. However:\n",
      "*   **Isolationism:** The United States remained isolationist, wanting to stay out of European conflicts.\n",
      "*   **Mistrust:** Western democracies viewed the Soviet Union (USSR) with suspicion.\n",
      "*   **The Shock Pact:** In August 1939, just days before the war began, Hitler signed a **Non-Aggression Pact** with Joseph Stalin. This agreement meant the two dictators agreed not to attack each other and secretly agreed to divide Poland between them. This ensured Hitler would not have to fight a two-front war immediately.\n",
      "\n",
      "### 6. The Immediate Cause: Invasion of Poland\n",
      "On **September 1, 1939**, German forces invaded Poland using \"Blitzkrieg\" (lightning war) tactics. Two days later, on **September 3, 1939**, Britain and France honored their guarantees to protect Poland and declared war on Germany, marking the official beginning of World War II in Europe.\n",
      "\n",
      "In summary, the war was caused by the destabilizing legacy of World War I, the aggressive expansionism of the Axis powers (Germany, Italy, Japan), and the inability of the Allied powers and international organizations to diplomatically stop that aggression.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent\n",
    "\n",
    "glm_model = LitellmModel(\n",
    "    model=\"huggingface/novita/zai-org/GLM-4.7\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    triage_agent,\n",
    "    \"Can you explain the causes of World War II?\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebdb44",
   "metadata": {},
   "source": [
    "#### Agents As Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2e5b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='to...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='World Wa...vided good response).'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The caus...of Poland's invasion.\"}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causes of World War II were complex and interconnected. Here's a comprehensive explanation:\n",
      "\n",
      "## The Main Causes of World War II\n",
      "\n",
      "### 1. The Legacy of World War I and the Treaty of Versailles\n",
      "The Treaty of Versailles (1919) that ended World War I imposed harsh terms on Germany:\n",
      "- Germany was forced to accept full blame for the war\n",
      "- Required to pay massive reparations\n",
      "- Lost significant territory (including Alsace-Lorraine and the Polish Corridor)\n",
      "- Military was severely restricted\n",
      "\n",
      "This humiliation created deep resentment among Germans, creating fertile ground for political extremism.\n",
      "\n",
      "### 2. The Rise of Totalitarian Regimes\n",
      "Three aggressive dictatorships emerged with expansionist ambitions:\n",
      "- **Germany (Nazi Germany):** Hitler sought to restore German pride and create *Lebensraum* (\"living space\") through conquest, particularly targeting Eastern Europe\n",
      "- **Italy (Fascist Italy):** Mussolini aimed to revive a Roman Empire controlling the Mediterranean\n",
      "- **Japan (Militarist Japan):** Lacking natural resources, Japan sought to dominate Asia and the Pacific through conquest, starting with Manchuria in 1931\n",
      "\n",
      "### 3. The Great Depression\n",
      "The 1929 economic collapse had devastating global effects:\n",
      "- High unemployment led people to extremist political solutions\n",
      "- Economic instability made nations more aggressive in seeking resources\n",
      "- Protectionist trade policies created international tensions\n",
      "\n",
      "### 4. The Failure of Appeasement\n",
      "Britain and France, desperate to avoid another war, adopted a policy of appeasement:\n",
      "- They allowed Germany to break the Treaty of Versailles (rearmament, Rhineland occupation)\n",
      "- The 1938 Munich Agreement let Germany take the Sudetenland from Czechoslovakia\n",
      "- These concessions only emboldened Hitler, leading him to occupy all of Czechoslovakia in 1939\n",
      "\n",
      "### 5. The Immediate Trigger: Invasion of Poland\n",
      "- Hitler demanded the return of Danzig and the Polish Corridor from Poland\n",
      "- In August 1939, Germany signed a Nazi-Soviet Pact (non-aggression agreement) to prevent a two-front war\n",
      "- On **September 1, 1939**, Germany invaded Poland using Blitzkrieg tactics\n",
      "- Britain and France declared war on Germany on September 3, 1939\n",
      "\n",
      "## Summary\n",
      "World War II began because totalitarian powers (Germany, Italy, and Japan) decided to expand their territories by force, while the democratic nations were too weakened by economic depression and too fearful of war to stop them until it was too late. The conflict became truly global when Japan attacked Pearl Harbor in 1941, drawing the United States into the war.\n",
      "\n",
      "Would you like me to elaborate on any specific aspect of these causes?\n"
     ]
    }
   ],
   "source": [
    "manager_agent = Agent(\n",
    "    name=\"Manager Agent\",\n",
    "    instructions=\"You manage a team of agents to answer user questions effectively.\",\n",
    "    tools=[\n",
    "        history_tutor_agent.as_tool(\n",
    "            tool_name=\"history_tutor_agent\",\n",
    "            tool_description=\"Handles historical queries\",\n",
    "        ),\n",
    "        math_tutor_agent.as_tool(\n",
    "            tool_name=\"math_tutor_agent\",\n",
    "            tool_description=\"Handles math questions\",\n",
    "        ),\n",
    "    ],\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    manager_agent,\n",
    "    \"Can you explain the causes of World War II?\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb128e37",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8e2426",
   "metadata": {},
   "source": [
    "### Pre-built Tools\n",
    "\n",
    "OpenAI offers a few built-in tools when using the OpenAIResponsesModel:\n",
    "\n",
    "- The `WebSearchTool` lets an agent search the web.\n",
    "- The `FileSearchTool` allows retrieving information from your OpenAI Vector Stores.\n",
    "- The `ComputerTool` allows automating computer use tasks.\n",
    "- The `CodeInterpreterTool` lets the LLM execute code in a sandboxed environment.\n",
    "- The `HostedMCPTool` exposes a remote MCP server's tools to the model.\n",
    "- The `ImageGenerationTool` generates images from a prompt.\n",
    "- The `LocalShellTool` runs shell commands on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c27453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of January 6, 2026 the President of the United States is Donald J. Trump (47th President), inaugurated January 20, 2025. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Second_presidency_of_Donald_Trump?utm_source=openai))\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, WebSearchTool\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    tools=[\n",
    "        WebSearchTool(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, \"Who is the current president of the United States as of 2026?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1353ac43",
   "metadata": {},
   "source": [
    "### Custom Tools\n",
    "\n",
    "You can define custom tools that the agent can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21850942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco is sunny!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of the file `/tmp/hello.txt` are:\n",
      "\n",
      "```\n",
      "Hello, World!\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "ERROR:openai.agents:[non-fatal] Tracing: max retries reached, giving up on this batch.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing_extensions import TypedDict, Any\n",
    "from agents import Agent, FunctionTool, RunContextWrapper, function_tool\n",
    "\n",
    "\n",
    "class Location(TypedDict):\n",
    "    lat: float\n",
    "    long: float\n",
    "\n",
    "@function_tool  \n",
    "async def fetch_weather(location: Location) -> str:\n",
    "    \n",
    "    \"\"\"Fetch the weather for a given location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to fetch the weather for.\n",
    "    \"\"\"\n",
    "    # In real life, we'd fetch the weather from a weather API\n",
    "    return f\"The weather in {location['lat']}, {location['long']} is sunny\"\n",
    "\n",
    "\n",
    "@function_tool(name_override=\"fetch_data\")  \n",
    "def read_file(path: str, directory: str | None = None) -> str:\n",
    "    \"\"\"Read the contents of a file.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the file to read.\n",
    "        directory: The directory to read the file from.\n",
    "    \"\"\"\n",
    "    # In real life, we'd read the file from the file system\n",
    "    return \"Hello, World!\"\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    tools=[fetch_weather, read_file],  \n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent, \"What is the weather in San Francisco?\")\n",
    "print(result.final_output)\n",
    "\n",
    "result = await Runner.run(agent, \"What are the contents of the file /tmp/hello.txt?\")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e976e",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "172ac28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='San Fran...ion:** San Francisco.'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco.\n",
      "California.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Californ...swer:** \"California.\"'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, SQLiteSession\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Reply very concisely.\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True)\n",
    ")\n",
    "\n",
    "# Create a new conversation\n",
    "session = SQLiteSession(session_id=\"conv_123\")\n",
    "\n",
    "# Optionally resume a previous conversation by passing a conversation ID\n",
    "# session = OpenAIConversationsSession(conversation_id=\"conv_123\")\n",
    "\n",
    "# Start conversation\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"What city is the Golden Gate Bridge in?\",\n",
    "    session=session\n",
    ")\n",
    "print(result.final_output)  # \"San Francisco\"\n",
    "\n",
    "# Continue the conversation\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"What state is it in?\",\n",
    "    session=session\n",
    ")\n",
    "print(result.final_output)  # \"California\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd09286",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04915265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 jokes for you:\n",
      "\n",
      "1.  **I told my wife she was drawing her eyebrows too high.**\n",
      "    She looked surprised.\n",
      "\n",
      "2.  **Why don't skeletons fight each other?**\n",
      "    They don't have the guts.\n",
      "\n",
      "3.  **What did the ocean say to the beach?**\n",
      "    Nothing, it just waved.\n",
      "\n",
      "4.  **Why did the scarecrow win an award?**\n",
      "    Because he was outstanding in his field.\n",
      "\n",
      "5.  **I threw a boomerang a few years ago.**\n",
      "    I now live in constant fear."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=\"Here are...er_specific_fields=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from agents import Agent, Runner\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=glm_model,\n",
    "        model_settings=ModelSettings(include_usage=True)\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f63877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
