{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432d503e",
   "metadata": {},
   "source": [
    "# OpenAI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3a271c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.4/249.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -Uq openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7aec0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.2 requires grpcio>=1.71.2, but you have grpcio 1.67.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# to use non-openai models, e.g., from Hugging Face\n",
    "!pip install -Uq \"openai-agents[litellm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable async in notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526c20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10393d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set default model for agents\n",
    "\n",
    "os.environ[\"OPENAI_DEFAULT_MODEL\"] = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20992c80",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"returns weather info for the specified city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Haiku agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[get_weather],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "result = await Runner.run(agent, \"What's the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1516495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'agents.result.RunResult'>\n",
      "{'_last_agent': Agent(name='Haiku agent',\n",
      "                      handoff_description=None,\n",
      "                      tools=[FunctionTool(name='get_weather',\n",
      "                                          description='returns weather info '\n",
      "                                                      'for the specified city.',\n",
      "                                          params_json_schema={'additionalProperties': False,\n",
      "                                                              'properties': {'city': {'title': 'City',\n",
      "                                                                                      'type': 'string'}},\n",
      "                                                              'required': ['city'],\n",
      "                                                              'title': 'get_weather_args',\n",
      "                                                              'type': 'object'},\n",
      "                                          on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                          strict_json_schema=True,\n",
      "                                          is_enabled=True,\n",
      "                                          tool_input_guardrails=None,\n",
      "                                          tool_output_guardrails=None)],\n",
      "                      mcp_servers=[],\n",
      "                      mcp_config={},\n",
      "                      instructions='Always respond in haiku form',\n",
      "                      prompt=None,\n",
      "                      handoffs=[],\n",
      "                      model='gpt-5-mini',\n",
      "                      model_settings=ModelSettings(temperature=None,\n",
      "                                                   top_p=None,\n",
      "                                                   frequency_penalty=None,\n",
      "                                                   presence_penalty=None,\n",
      "                                                   tool_choice=None,\n",
      "                                                   parallel_tool_calls=None,\n",
      "                                                   truncation=None,\n",
      "                                                   max_tokens=None,\n",
      "                                                   reasoning=None,\n",
      "                                                   verbosity=None,\n",
      "                                                   metadata=None,\n",
      "                                                   store=None,\n",
      "                                                   prompt_cache_retention=None,\n",
      "                                                   include_usage=None,\n",
      "                                                   response_include=None,\n",
      "                                                   top_logprobs=None,\n",
      "                                                   extra_query=None,\n",
      "                                                   extra_body=None,\n",
      "                                                   extra_headers=None,\n",
      "                                                   extra_args=None),\n",
      "                      input_guardrails=[],\n",
      "                      output_guardrails=[],\n",
      "                      output_type=None,\n",
      "                      hooks=None,\n",
      "                      tool_use_behavior='run_llm_again',\n",
      "                      reset_tool_choice=True),\n",
      " '_last_agent_ref': <weakref at 0x7c89f96aaf70; to 'Agent' at 0x7c89fb1d4e60>,\n",
      " 'context_wrapper': RunContextWrapper(context=None,\n",
      "                                      usage=Usage(requests=2,\n",
      "                                                  input_tokens=273,\n",
      "                                                  input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                                  output_tokens=494,\n",
      "                                                  output_tokens_details=OutputTokensDetails(reasoning_tokens=448),\n",
      "                                                  total_tokens=767,\n",
      "                                                  request_usage_entries=[RequestUsage(input_tokens=69,\n",
      "                                                                                      output_tokens=85,\n",
      "                                                                                      total_tokens=154,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=64)),\n",
      "                                                                         RequestUsage(input_tokens=204,\n",
      "                                                                                      output_tokens=409,\n",
      "                                                                                      total_tokens=613,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=384))])),\n",
      " 'final_output': 'Sun warms city streets  \\n'\n",
      "                 'clear blue sky, no cloud in sight  \\n'\n",
      "                 'Smiles bloom on faces',\n",
      " 'input': \"What's the weather in New York?\",\n",
      " 'input_guardrail_results': [],\n",
      " 'new_items': [ReasoningItem(agent=Agent(name='Haiku agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model='gpt-5-mini',\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=None,\n",
      "                                                                      verbosity=None,\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=None,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='rs_0df9dd2eafa28b4e00695d4db4e2bc8193b618fc7346c8bead', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                             type='reasoning_item'),\n",
      "               ToolCallItem(agent=Agent(name='Haiku agent',\n",
      "                                        handoff_description=None,\n",
      "                                        tools=[FunctionTool(name='get_weather',\n",
      "                                                            description='returns '\n",
      "                                                                        'weather '\n",
      "                                                                        'info '\n",
      "                                                                        'for '\n",
      "                                                                        'the '\n",
      "                                                                        'specified '\n",
      "                                                                        'city.',\n",
      "                                                            params_json_schema={'additionalProperties': False,\n",
      "                                                                                'properties': {'city': {'title': 'City',\n",
      "                                                                                                        'type': 'string'}},\n",
      "                                                                                'required': ['city'],\n",
      "                                                                                'title': 'get_weather_args',\n",
      "                                                                                'type': 'object'},\n",
      "                                                            on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                            strict_json_schema=True,\n",
      "                                                            is_enabled=True,\n",
      "                                                            tool_input_guardrails=None,\n",
      "                                                            tool_output_guardrails=None)],\n",
      "                                        mcp_servers=[],\n",
      "                                        mcp_config={},\n",
      "                                        instructions='Always respond in haiku '\n",
      "                                                     'form',\n",
      "                                        prompt=None,\n",
      "                                        handoffs=[],\n",
      "                                        model='gpt-5-mini',\n",
      "                                        model_settings=ModelSettings(temperature=None,\n",
      "                                                                     top_p=None,\n",
      "                                                                     frequency_penalty=None,\n",
      "                                                                     presence_penalty=None,\n",
      "                                                                     tool_choice=None,\n",
      "                                                                     parallel_tool_calls=None,\n",
      "                                                                     truncation=None,\n",
      "                                                                     max_tokens=None,\n",
      "                                                                     reasoning=None,\n",
      "                                                                     verbosity=None,\n",
      "                                                                     metadata=None,\n",
      "                                                                     store=None,\n",
      "                                                                     prompt_cache_retention=None,\n",
      "                                                                     include_usage=None,\n",
      "                                                                     response_include=None,\n",
      "                                                                     top_logprobs=None,\n",
      "                                                                     extra_query=None,\n",
      "                                                                     extra_body=None,\n",
      "                                                                     extra_headers=None,\n",
      "                                                                     extra_args=None),\n",
      "                                        input_guardrails=[],\n",
      "                                        output_guardrails=[],\n",
      "                                        output_type=None,\n",
      "                                        hooks=None,\n",
      "                                        tool_use_behavior='run_llm_again',\n",
      "                                        reset_tool_choice=True),\n",
      "                            raw_item=ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_OETt0JeRMo9loR3ICPBuFXHH', name='get_weather', type='function_call', id='fc_0df9dd2eafa28b4e00695d4db67ca88193802807c55f6c65c3', status='completed'),\n",
      "                            type='tool_call_item'),\n",
      "               ToolCallOutputItem(agent=Agent(name='Haiku agent',\n",
      "                                              handoff_description=None,\n",
      "                                              tools=[FunctionTool(name='get_weather',\n",
      "                                                                  description='returns '\n",
      "                                                                              'weather '\n",
      "                                                                              'info '\n",
      "                                                                              'for '\n",
      "                                                                              'the '\n",
      "                                                                              'specified '\n",
      "                                                                              'city.',\n",
      "                                                                  params_json_schema={'additionalProperties': False,\n",
      "                                                                                      'properties': {'city': {'title': 'City',\n",
      "                                                                                                              'type': 'string'}},\n",
      "                                                                                      'required': ['city'],\n",
      "                                                                                      'title': 'get_weather_args',\n",
      "                                                                                      'type': 'object'},\n",
      "                                                                  on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                                  strict_json_schema=True,\n",
      "                                                                  is_enabled=True,\n",
      "                                                                  tool_input_guardrails=None,\n",
      "                                                                  tool_output_guardrails=None)],\n",
      "                                              mcp_servers=[],\n",
      "                                              mcp_config={},\n",
      "                                              instructions='Always respond in '\n",
      "                                                           'haiku form',\n",
      "                                              prompt=None,\n",
      "                                              handoffs=[],\n",
      "                                              model='gpt-5-mini',\n",
      "                                              model_settings=ModelSettings(temperature=None,\n",
      "                                                                           top_p=None,\n",
      "                                                                           frequency_penalty=None,\n",
      "                                                                           presence_penalty=None,\n",
      "                                                                           tool_choice=None,\n",
      "                                                                           parallel_tool_calls=None,\n",
      "                                                                           truncation=None,\n",
      "                                                                           max_tokens=None,\n",
      "                                                                           reasoning=None,\n",
      "                                                                           verbosity=None,\n",
      "                                                                           metadata=None,\n",
      "                                                                           store=None,\n",
      "                                                                           prompt_cache_retention=None,\n",
      "                                                                           include_usage=None,\n",
      "                                                                           response_include=None,\n",
      "                                                                           top_logprobs=None,\n",
      "                                                                           extra_query=None,\n",
      "                                                                           extra_body=None,\n",
      "                                                                           extra_headers=None,\n",
      "                                                                           extra_args=None),\n",
      "                                              input_guardrails=[],\n",
      "                                              output_guardrails=[],\n",
      "                                              output_type=None,\n",
      "                                              hooks=None,\n",
      "                                              tool_use_behavior='run_llm_again',\n",
      "                                              reset_tool_choice=True),\n",
      "                                  raw_item={'call_id': 'call_OETt0JeRMo9loR3ICPBuFXHH',\n",
      "                                            'output': 'The weather in New York '\n",
      "                                                      'is sunny',\n",
      "                                            'type': 'function_call_output'},\n",
      "                                  output='The weather in New York is sunny',\n",
      "                                  type='tool_call_output_item'),\n",
      "               ReasoningItem(agent=Agent(name='Haiku agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model='gpt-5-mini',\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=None,\n",
      "                                                                      verbosity=None,\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=None,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='rs_0df9dd2eafa28b4e00695d4db8144c819383b218df551cd1ce', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                             type='reasoning_item'),\n",
      "               MessageOutputItem(agent=Agent(name='Haiku agent',\n",
      "                                             handoff_description=None,\n",
      "                                             tools=[FunctionTool(name='get_weather',\n",
      "                                                                 description='returns '\n",
      "                                                                             'weather '\n",
      "                                                                             'info '\n",
      "                                                                             'for '\n",
      "                                                                             'the '\n",
      "                                                                             'specified '\n",
      "                                                                             'city.',\n",
      "                                                                 params_json_schema={'additionalProperties': False,\n",
      "                                                                                     'properties': {'city': {'title': 'City',\n",
      "                                                                                                             'type': 'string'}},\n",
      "                                                                                     'required': ['city'],\n",
      "                                                                                     'title': 'get_weather_args',\n",
      "                                                                                     'type': 'object'},\n",
      "                                                                 on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                                 strict_json_schema=True,\n",
      "                                                                 is_enabled=True,\n",
      "                                                                 tool_input_guardrails=None,\n",
      "                                                                 tool_output_guardrails=None)],\n",
      "                                             mcp_servers=[],\n",
      "                                             mcp_config={},\n",
      "                                             instructions='Always respond in '\n",
      "                                                          'haiku form',\n",
      "                                             prompt=None,\n",
      "                                             handoffs=[],\n",
      "                                             model='gpt-5-mini',\n",
      "                                             model_settings=ModelSettings(temperature=None,\n",
      "                                                                          top_p=None,\n",
      "                                                                          frequency_penalty=None,\n",
      "                                                                          presence_penalty=None,\n",
      "                                                                          tool_choice=None,\n",
      "                                                                          parallel_tool_calls=None,\n",
      "                                                                          truncation=None,\n",
      "                                                                          max_tokens=None,\n",
      "                                                                          reasoning=None,\n",
      "                                                                          verbosity=None,\n",
      "                                                                          metadata=None,\n",
      "                                                                          store=None,\n",
      "                                                                          prompt_cache_retention=None,\n",
      "                                                                          include_usage=None,\n",
      "                                                                          response_include=None,\n",
      "                                                                          top_logprobs=None,\n",
      "                                                                          extra_query=None,\n",
      "                                                                          extra_body=None,\n",
      "                                                                          extra_headers=None,\n",
      "                                                                          extra_args=None),\n",
      "                                             input_guardrails=[],\n",
      "                                             output_guardrails=[],\n",
      "                                             output_type=None,\n",
      "                                             hooks=None,\n",
      "                                             tool_use_behavior='run_llm_again',\n",
      "                                             reset_tool_choice=True),\n",
      "                                 raw_item=ResponseOutputMessage(id='msg_0df9dd2eafa28b4e00695d4dc1a4308193850e8e395b4647c3', content=[ResponseOutputText(annotations=[], text='Sun warms city streets  \\nclear blue sky, no cloud in sight  \\nSmiles bloom on faces', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
      "                                 type='message_output_item')],\n",
      " 'output_guardrail_results': [],\n",
      " 'raw_responses': [ModelResponse(output=[ResponseReasoningItem(id='rs_0df9dd2eafa28b4e00695d4db4e2bc8193b618fc7346c8bead', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                                         ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_OETt0JeRMo9loR3ICPBuFXHH', name='get_weather', type='function_call', id='fc_0df9dd2eafa28b4e00695d4db67ca88193802807c55f6c65c3', status='completed')],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=69,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                             output_tokens=85,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=64),\n",
      "                                             total_tokens=154,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id='resp_0df9dd2eafa28b4e00695d4db4675c8193802187d62753854f'),\n",
      "                   ModelResponse(output=[ResponseReasoningItem(id='rs_0df9dd2eafa28b4e00695d4db8144c819383b218df551cd1ce', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
      "                                         ResponseOutputMessage(id='msg_0df9dd2eafa28b4e00695d4dc1a4308193850e8e395b4647c3', content=[ResponseOutputText(annotations=[], text='Sun warms city streets  \\nclear blue sky, no cloud in sight  \\nSmiles bloom on faces', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=204,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=0),\n",
      "                                             output_tokens=409,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=384),\n",
      "                                             total_tokens=613,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id='resp_0df9dd2eafa28b4e00695d4db79c848193bea49d40dfc738c7')],\n",
      " 'tool_input_guardrail_results': [],\n",
      " 'tool_output_guardrail_results': []}\n"
     ]
    }
   ],
   "source": [
    "print(type(result))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result.__dict__, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9db2a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun warms city streets  \n",
      "clear blue sky, no cloud in sight  \n",
      "Smiles bloom on faces\n"
     ]
    }
   ],
   "source": [
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcf5d8",
   "metadata": {},
   "source": [
    "## Non Open-AI models\n",
    "\n",
    "Use Hugging Face models with OpenAI Agents SDK.\n",
    "\n",
    "Just add the HF_TOKEN env variable. And set the model param to:\n",
    "\n",
    "```\n",
    "litellm/huggingface/<provider>/<hf_org_or_user>/<hf_model>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e4f0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kimi-k2-thinking\n",
    "\n",
    "from agents import Agent, Runner, ModelSettings\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "\n",
    "agent_kimi = Agent(\n",
    "    name=\"Kimi agent\",\n",
    "    instructions=\"Always respond in haiku form\",\n",
    "    # model=\"litellm/huggingface/novita/MiniMaxAI/MiniMax-M2.1\",\n",
    "    tools=[get_weather],\n",
    "    \n",
    "    # optional, for tracing (requires openai API key)\n",
    "    model=LitellmModel(model=\"huggingface/novita/MiniMaxAI/MiniMax-M2.1\", api_key=os.environ[\"HF_TOKEN\"]),\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent_kimi, \"What's the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1cfd9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_last_agent': Agent(name='Kimi agent',\n",
      "                      handoff_description=None,\n",
      "                      tools=[FunctionTool(name='get_weather',\n",
      "                                          description='returns weather info '\n",
      "                                                      'for the specified city.',\n",
      "                                          params_json_schema={'additionalProperties': False,\n",
      "                                                              'properties': {'city': {'title': 'City',\n",
      "                                                                                      'type': 'string'}},\n",
      "                                                              'required': ['city'],\n",
      "                                                              'title': 'get_weather_args',\n",
      "                                                              'type': 'object'},\n",
      "                                          on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                          strict_json_schema=True,\n",
      "                                          is_enabled=True,\n",
      "                                          tool_input_guardrails=None,\n",
      "                                          tool_output_guardrails=None)],\n",
      "                      mcp_servers=[],\n",
      "                      mcp_config={},\n",
      "                      instructions='Always respond in haiku form',\n",
      "                      prompt=None,\n",
      "                      handoffs=[],\n",
      "                      model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7c89e938e6f0>,\n",
      "                      model_settings=ModelSettings(temperature=None,\n",
      "                                                   top_p=None,\n",
      "                                                   frequency_penalty=None,\n",
      "                                                   presence_penalty=None,\n",
      "                                                   tool_choice=None,\n",
      "                                                   parallel_tool_calls=None,\n",
      "                                                   truncation=None,\n",
      "                                                   max_tokens=None,\n",
      "                                                   reasoning=None,\n",
      "                                                   verbosity=None,\n",
      "                                                   metadata=None,\n",
      "                                                   store=None,\n",
      "                                                   prompt_cache_retention=None,\n",
      "                                                   include_usage=True,\n",
      "                                                   response_include=None,\n",
      "                                                   top_logprobs=None,\n",
      "                                                   extra_query=None,\n",
      "                                                   extra_body=None,\n",
      "                                                   extra_headers=None,\n",
      "                                                   extra_args=None),\n",
      "                      input_guardrails=[],\n",
      "                      output_guardrails=[],\n",
      "                      output_type=None,\n",
      "                      hooks=None,\n",
      "                      tool_use_behavior='run_llm_again',\n",
      "                      reset_tool_choice=True),\n",
      " '_last_agent_ref': <weakref at 0x7c89e9b81990; to 'Agent' at 0x7c89e938e840>,\n",
      " 'context_wrapper': RunContextWrapper(context=None,\n",
      "                                      usage=Usage(requests=2,\n",
      "                                                  input_tokens=415,\n",
      "                                                  input_tokens_details=InputTokensDetails(cached_tokens=415),\n",
      "                                                  output_tokens=1170,\n",
      "                                                  output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                                  total_tokens=1585,\n",
      "                                                  request_usage_entries=[RequestUsage(input_tokens=184,\n",
      "                                                                                      output_tokens=341,\n",
      "                                                                                      total_tokens=525,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=184),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=0)),\n",
      "                                                                         RequestUsage(input_tokens=231,\n",
      "                                                                                      output_tokens=829,\n",
      "                                                                                      total_tokens=1060,\n",
      "                                                                                      input_tokens_details=InputTokensDetails(cached_tokens=231),\n",
      "                                                                                      output_tokens_details=OutputTokensDetails(reasoning_tokens=0))])),\n",
      " 'final_output': 'Sunny New York day\\n'\n",
      "                 'Golden light fills the streets\\n'\n",
      "                 'Summer warmth embraces',\n",
      " 'input': \"What's the weather in New York?\",\n",
      " 'input_guardrail_results': [],\n",
      " 'new_items': [ReasoningItem(agent=Agent(name='Kimi agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7c89e938e6f0>,\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=None,\n",
      "                                                                      verbosity=None,\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=True,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='Okay, so the user is asking about the weather in New York. This is a straightforward request that requires me to provide current weather information for a specific location.\\n\\nI notice that I have access to a tool called \"get_weather\" which seems perfect for this situation. Looking at the tool description, it \"returns weather info for the specified city.\" This is exactly what I need to answer the user\\'s question accurately.\\n\\nThe tool requires a parameter called \"city\" which should be a string. In this case, the user has clearly specified they want the weather for \"New York\" - this is a valid city name that I can directly pass to the tool.\\n\\nSince I don\\'t have direct access to real-time weather data without using the provided tool, I need to make a tool call to get this information. The proper way to do this is to use the specified format with XML tags and JSON objects.\\n\\nI need to structure my response with:\\n1. The tool_calls opening tag\\n2. The name of the tool: \"get_weather\"\\n3. The arguments as a JSON object: {\"city\": \"New York\"}\\n4. The closing tool_calls tag\\n\\nThis will invoke the weather tool and retrieve the current weather information for New York City, which I can then relay back to the user. The tool will handle the actual data retrieval, and once I get the response, I\\'ll be able to provide the user with the weather information they requested.\\n\\nSo my response should be a properly formatted tool call to get_weather with New York as the city parameter.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '07d014cfd5e4235692cb5badb7488181'}),\n",
      "                             type='reasoning_item'),\n",
      "               ToolCallItem(agent=Agent(name='Kimi agent',\n",
      "                                        handoff_description=None,\n",
      "                                        tools=[FunctionTool(name='get_weather',\n",
      "                                                            description='returns '\n",
      "                                                                        'weather '\n",
      "                                                                        'info '\n",
      "                                                                        'for '\n",
      "                                                                        'the '\n",
      "                                                                        'specified '\n",
      "                                                                        'city.',\n",
      "                                                            params_json_schema={'additionalProperties': False,\n",
      "                                                                                'properties': {'city': {'title': 'City',\n",
      "                                                                                                        'type': 'string'}},\n",
      "                                                                                'required': ['city'],\n",
      "                                                                                'title': 'get_weather_args',\n",
      "                                                                                'type': 'object'},\n",
      "                                                            on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                            strict_json_schema=True,\n",
      "                                                            is_enabled=True,\n",
      "                                                            tool_input_guardrails=None,\n",
      "                                                            tool_output_guardrails=None)],\n",
      "                                        mcp_servers=[],\n",
      "                                        mcp_config={},\n",
      "                                        instructions='Always respond in haiku '\n",
      "                                                     'form',\n",
      "                                        prompt=None,\n",
      "                                        handoffs=[],\n",
      "                                        model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7c89e938e6f0>,\n",
      "                                        model_settings=ModelSettings(temperature=None,\n",
      "                                                                     top_p=None,\n",
      "                                                                     frequency_penalty=None,\n",
      "                                                                     presence_penalty=None,\n",
      "                                                                     tool_choice=None,\n",
      "                                                                     parallel_tool_calls=None,\n",
      "                                                                     truncation=None,\n",
      "                                                                     max_tokens=None,\n",
      "                                                                     reasoning=None,\n",
      "                                                                     verbosity=None,\n",
      "                                                                     metadata=None,\n",
      "                                                                     store=None,\n",
      "                                                                     prompt_cache_retention=None,\n",
      "                                                                     include_usage=True,\n",
      "                                                                     response_include=None,\n",
      "                                                                     top_logprobs=None,\n",
      "                                                                     extra_query=None,\n",
      "                                                                     extra_body=None,\n",
      "                                                                     extra_headers=None,\n",
      "                                                                     extra_args=None),\n",
      "                                        input_guardrails=[],\n",
      "                                        output_guardrails=[],\n",
      "                                        output_type=None,\n",
      "                                        hooks=None,\n",
      "                                        tool_use_behavior='run_llm_again',\n",
      "                                        reset_tool_choice=True),\n",
      "                            raw_item=ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_function_bsvdsh67kzkv_1', name='get_weather', type='function_call', id='__fake_id__', status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '07d014cfd5e4235692cb5badb7488181'}),\n",
      "                            type='tool_call_item'),\n",
      "               ToolCallOutputItem(agent=Agent(name='Kimi agent',\n",
      "                                              handoff_description=None,\n",
      "                                              tools=[FunctionTool(name='get_weather',\n",
      "                                                                  description='returns '\n",
      "                                                                              'weather '\n",
      "                                                                              'info '\n",
      "                                                                              'for '\n",
      "                                                                              'the '\n",
      "                                                                              'specified '\n",
      "                                                                              'city.',\n",
      "                                                                  params_json_schema={'additionalProperties': False,\n",
      "                                                                                      'properties': {'city': {'title': 'City',\n",
      "                                                                                                              'type': 'string'}},\n",
      "                                                                                      'required': ['city'],\n",
      "                                                                                      'title': 'get_weather_args',\n",
      "                                                                                      'type': 'object'},\n",
      "                                                                  on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                                  strict_json_schema=True,\n",
      "                                                                  is_enabled=True,\n",
      "                                                                  tool_input_guardrails=None,\n",
      "                                                                  tool_output_guardrails=None)],\n",
      "                                              mcp_servers=[],\n",
      "                                              mcp_config={},\n",
      "                                              instructions='Always respond in '\n",
      "                                                           'haiku form',\n",
      "                                              prompt=None,\n",
      "                                              handoffs=[],\n",
      "                                              model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7c89e938e6f0>,\n",
      "                                              model_settings=ModelSettings(temperature=None,\n",
      "                                                                           top_p=None,\n",
      "                                                                           frequency_penalty=None,\n",
      "                                                                           presence_penalty=None,\n",
      "                                                                           tool_choice=None,\n",
      "                                                                           parallel_tool_calls=None,\n",
      "                                                                           truncation=None,\n",
      "                                                                           max_tokens=None,\n",
      "                                                                           reasoning=None,\n",
      "                                                                           verbosity=None,\n",
      "                                                                           metadata=None,\n",
      "                                                                           store=None,\n",
      "                                                                           prompt_cache_retention=None,\n",
      "                                                                           include_usage=True,\n",
      "                                                                           response_include=None,\n",
      "                                                                           top_logprobs=None,\n",
      "                                                                           extra_query=None,\n",
      "                                                                           extra_body=None,\n",
      "                                                                           extra_headers=None,\n",
      "                                                                           extra_args=None),\n",
      "                                              input_guardrails=[],\n",
      "                                              output_guardrails=[],\n",
      "                                              output_type=None,\n",
      "                                              hooks=None,\n",
      "                                              tool_use_behavior='run_llm_again',\n",
      "                                              reset_tool_choice=True),\n",
      "                                  raw_item={'call_id': 'call_function_bsvdsh67kzkv_1',\n",
      "                                            'output': 'The weather in New York '\n",
      "                                                      'is sunny',\n",
      "                                            'type': 'function_call_output'},\n",
      "                                  output='The weather in New York is sunny',\n",
      "                                  type='tool_call_output_item'),\n",
      "               ReasoningItem(agent=Agent(name='Kimi agent',\n",
      "                                         handoff_description=None,\n",
      "                                         tools=[FunctionTool(name='get_weather',\n",
      "                                                             description='returns '\n",
      "                                                                         'weather '\n",
      "                                                                         'info '\n",
      "                                                                         'for '\n",
      "                                                                         'the '\n",
      "                                                                         'specified '\n",
      "                                                                         'city.',\n",
      "                                                             params_json_schema={'additionalProperties': False,\n",
      "                                                                                 'properties': {'city': {'title': 'City',\n",
      "                                                                                                         'type': 'string'}},\n",
      "                                                                                 'required': ['city'],\n",
      "                                                                                 'title': 'get_weather_args',\n",
      "                                                                                 'type': 'object'},\n",
      "                                                             on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                             strict_json_schema=True,\n",
      "                                                             is_enabled=True,\n",
      "                                                             tool_input_guardrails=None,\n",
      "                                                             tool_output_guardrails=None)],\n",
      "                                         mcp_servers=[],\n",
      "                                         mcp_config={},\n",
      "                                         instructions='Always respond in haiku '\n",
      "                                                      'form',\n",
      "                                         prompt=None,\n",
      "                                         handoffs=[],\n",
      "                                         model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7c89e938e6f0>,\n",
      "                                         model_settings=ModelSettings(temperature=None,\n",
      "                                                                      top_p=None,\n",
      "                                                                      frequency_penalty=None,\n",
      "                                                                      presence_penalty=None,\n",
      "                                                                      tool_choice=None,\n",
      "                                                                      parallel_tool_calls=None,\n",
      "                                                                      truncation=None,\n",
      "                                                                      max_tokens=None,\n",
      "                                                                      reasoning=None,\n",
      "                                                                      verbosity=None,\n",
      "                                                                      metadata=None,\n",
      "                                                                      store=None,\n",
      "                                                                      prompt_cache_retention=None,\n",
      "                                                                      include_usage=True,\n",
      "                                                                      response_include=None,\n",
      "                                                                      top_logprobs=None,\n",
      "                                                                      extra_query=None,\n",
      "                                                                      extra_body=None,\n",
      "                                                                      extra_headers=None,\n",
      "                                                                      extra_args=None),\n",
      "                                         input_guardrails=[],\n",
      "                                         output_guardrails=[],\n",
      "                                         output_type=None,\n",
      "                                         hooks=None,\n",
      "                                         tool_use_behavior='run_llm_again',\n",
      "                                         reset_tool_choice=True),\n",
      "                             raw_item=ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='The user asked about the weather in New York, and I used the get_weather tool to find out. The tool returned that the weather in New York is sunny. Now I need to provide this information to the user in haiku form, as instructed.\\n\\nLet me think about how to structure a haiku about sunny weather in New York:\\n\\nHaiku structure:\\n- Line 1: 5 syllables\\n- Line 2: 7 syllables\\n- Line 3: 5 syllables\\n\\nLet me try a few options:\\n\\nOption 1:\\n\"Sunny New York day\" (5 syllables - Sun-ny New York day)\\n\"Buildings gleam in warm light\" (7 syllables - Build-ings gleam in warm light - that\\'s 6, need 7)\\nMaybe \"Buildings gleam in bright light\" (7 syllables - Build-ings gleam in bright light)\\n\"Blue sky stretches wide\" (5 syllables - Blue sky stretch-es wide - that\\'s 5)\\n\\nLet me check:\\n- \"Sunny New York day\" = Sun-ny (2) New (1) York (1) day (1) = 5 ✓\\n- \"Buildings gleam in bright light\" = Build-ings (2) gleam (1) in (1) bright (1) light (1) = 6 ✗\\n- \"Blue sky stretches wide\" = Blue (1) sky (1) stretch-es (2) wide (1) = 5 ✓\\n\\nLet me try again:\\nOption 2:\\n\"Golden sun high above\" (5 syllables - Gold-en sun high a-bove = 6, too many)\\n\"New York City gleams\" (5 syllables - New York Ci-ty gleams = 5)\\n\"Summer warmth embraced\" (5 syllables - Sum-mer warmth em-braced = 5)\\n\\nLet me count more carefully:\\n\"Golden sun high above\" = Gold-en (2) sun (1) high (1) a-bove (2) = 6 ✗\\n\"New York City gleams\" = New (1) York (1) Ci-ty (2) gleams (1) = 5 ✓\\n\\n\\n\"Summer warmth embraced\" = Sum-mer (2) warmth (1) em-braced (2) = 5 ✓\\n\\nI\\'m struggling with the syllable count for the first line. I\\'ll keep experimenting to find a 5-syllable line that captures the sunny New York atmosphere.\\n\\n\"City glows warmly bright\" looks promising. I\\'ll carefully count each syllable to ensure accuracy. Breaking down \"City glows warmly bright\" reveals a potential solution, with each word contributing to the precise 5-syllable structure I need.\\n\\nI\\'ll continue exploring variations that might work within the haiku\\'s strict rhythmic constraints. The challenge is finding the right balance between imagery and syllable count.\\n\\nMy next step is to test different phrasings that could potentially fit the 5-7-5 syllable pattern, focusing on capturing the essence of a sunny day in New York while maintaining the precise syllabic structure.\\n\\nI\\'ll try a haiku that emphasizes the brightness and urban landscape, carefully counting syllables to ensure each line meets the traditional requirements. The challenge is to create imagery that feels both vivid and concise, highlighting the interplay between sunlight and cityscape.\\n\\nMy current draft explores how to distill the feeling of a bright urban day into a precise poetic form, paying close attention to the rhythmic and syllabic constraints of the haiku style.\\n\\nThe lines suggest a sensory experience of sunlight illuminating architectural elements, with a subtle progression from light to atmospheric conditions. Each word choice aims to capture a moment of urban luminosity.\\n\\nThe syllabic structure reveals precise attention to rhythmic constraints, carefully mapping each syllabic stress to maintain poetic form while conveying a vivid image of New York City\\'s atmospheric brightness.\\n\\nI\\'ll craft a concise response highlighting the urban luminescence, capturing the essence of a sunny day through a carefully structured haiku that reflects the tool\\'s concise, informative output.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'dc121fdb476a27dc6cfcdf1e808e6190'}),\n",
      "                             type='reasoning_item'),\n",
      "               MessageOutputItem(agent=Agent(name='Kimi agent',\n",
      "                                             handoff_description=None,\n",
      "                                             tools=[FunctionTool(name='get_weather',\n",
      "                                                                 description='returns '\n",
      "                                                                             'weather '\n",
      "                                                                             'info '\n",
      "                                                                             'for '\n",
      "                                                                             'the '\n",
      "                                                                             'specified '\n",
      "                                                                             'city.',\n",
      "                                                                 params_json_schema={'additionalProperties': False,\n",
      "                                                                                     'properties': {'city': {'title': 'City',\n",
      "                                                                                                             'type': 'string'}},\n",
      "                                                                                     'required': ['city'],\n",
      "                                                                                     'title': 'get_weather_args',\n",
      "                                                                                     'type': 'object'},\n",
      "                                                                 on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7c89fafaa520>,\n",
      "                                                                 strict_json_schema=True,\n",
      "                                                                 is_enabled=True,\n",
      "                                                                 tool_input_guardrails=None,\n",
      "                                                                 tool_output_guardrails=None)],\n",
      "                                             mcp_servers=[],\n",
      "                                             mcp_config={},\n",
      "                                             instructions='Always respond in '\n",
      "                                                          'haiku form',\n",
      "                                             prompt=None,\n",
      "                                             handoffs=[],\n",
      "                                             model=<agents.extensions.models.litellm_model.LitellmModel object at 0x7c89e938e6f0>,\n",
      "                                             model_settings=ModelSettings(temperature=None,\n",
      "                                                                          top_p=None,\n",
      "                                                                          frequency_penalty=None,\n",
      "                                                                          presence_penalty=None,\n",
      "                                                                          tool_choice=None,\n",
      "                                                                          parallel_tool_calls=None,\n",
      "                                                                          truncation=None,\n",
      "                                                                          max_tokens=None,\n",
      "                                                                          reasoning=None,\n",
      "                                                                          verbosity=None,\n",
      "                                                                          metadata=None,\n",
      "                                                                          store=None,\n",
      "                                                                          prompt_cache_retention=None,\n",
      "                                                                          include_usage=True,\n",
      "                                                                          response_include=None,\n",
      "                                                                          top_logprobs=None,\n",
      "                                                                          extra_query=None,\n",
      "                                                                          extra_body=None,\n",
      "                                                                          extra_headers=None,\n",
      "                                                                          extra_args=None),\n",
      "                                             input_guardrails=[],\n",
      "                                             output_guardrails=[],\n",
      "                                             output_type=None,\n",
      "                                             hooks=None,\n",
      "                                             tool_use_behavior='run_llm_again',\n",
      "                                             reset_tool_choice=True),\n",
      "                                 raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Sunny New York day\\nGolden light fills the streets\\nSummer warmth embraces', type='output_text', logprobs=[])], role='assistant', status='completed', type='message', provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'dc121fdb476a27dc6cfcdf1e808e6190'}),\n",
      "                                 type='message_output_item')],\n",
      " 'output_guardrail_results': [],\n",
      " 'raw_responses': [ModelResponse(output=[ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='Okay, so the user is asking about the weather in New York. This is a straightforward request that requires me to provide current weather information for a specific location.\\n\\nI notice that I have access to a tool called \"get_weather\" which seems perfect for this situation. Looking at the tool description, it \"returns weather info for the specified city.\" This is exactly what I need to answer the user\\'s question accurately.\\n\\nThe tool requires a parameter called \"city\" which should be a string. In this case, the user has clearly specified they want the weather for \"New York\" - this is a valid city name that I can directly pass to the tool.\\n\\nSince I don\\'t have direct access to real-time weather data without using the provided tool, I need to make a tool call to get this information. The proper way to do this is to use the specified format with XML tags and JSON objects.\\n\\nI need to structure my response with:\\n1. The tool_calls opening tag\\n2. The name of the tool: \"get_weather\"\\n3. The arguments as a JSON object: {\"city\": \"New York\"}\\n4. The closing tool_calls tag\\n\\nThis will invoke the weather tool and retrieve the current weather information for New York City, which I can then relay back to the user. The tool will handle the actual data retrieval, and once I get the response, I\\'ll be able to provide the user with the weather information they requested.\\n\\nSo my response should be a properly formatted tool call to get_weather with New York as the city parameter.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '07d014cfd5e4235692cb5badb7488181'}),\n",
      "                                         ResponseFunctionToolCall(arguments='{\"city\":\"New York\"}', call_id='call_function_bsvdsh67kzkv_1', name='get_weather', type='function_call', id='__fake_id__', status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': '07d014cfd5e4235692cb5badb7488181'})],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=184,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=184),\n",
      "                                             output_tokens=341,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                             total_tokens=525,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id=None),\n",
      "                   ModelResponse(output=[ResponseReasoningItem(id='__fake_id__', summary=[Summary(text='The user asked about the weather in New York, and I used the get_weather tool to find out. The tool returned that the weather in New York is sunny. Now I need to provide this information to the user in haiku form, as instructed.\\n\\nLet me think about how to structure a haiku about sunny weather in New York:\\n\\nHaiku structure:\\n- Line 1: 5 syllables\\n- Line 2: 7 syllables\\n- Line 3: 5 syllables\\n\\nLet me try a few options:\\n\\nOption 1:\\n\"Sunny New York day\" (5 syllables - Sun-ny New York day)\\n\"Buildings gleam in warm light\" (7 syllables - Build-ings gleam in warm light - that\\'s 6, need 7)\\nMaybe \"Buildings gleam in bright light\" (7 syllables - Build-ings gleam in bright light)\\n\"Blue sky stretches wide\" (5 syllables - Blue sky stretch-es wide - that\\'s 5)\\n\\nLet me check:\\n- \"Sunny New York day\" = Sun-ny (2) New (1) York (1) day (1) = 5 ✓\\n- \"Buildings gleam in bright light\" = Build-ings (2) gleam (1) in (1) bright (1) light (1) = 6 ✗\\n- \"Blue sky stretches wide\" = Blue (1) sky (1) stretch-es (2) wide (1) = 5 ✓\\n\\nLet me try again:\\nOption 2:\\n\"Golden sun high above\" (5 syllables - Gold-en sun high a-bove = 6, too many)\\n\"New York City gleams\" (5 syllables - New York Ci-ty gleams = 5)\\n\"Summer warmth embraced\" (5 syllables - Sum-mer warmth em-braced = 5)\\n\\nLet me count more carefully:\\n\"Golden sun high above\" = Gold-en (2) sun (1) high (1) a-bove (2) = 6 ✗\\n\"New York City gleams\" = New (1) York (1) Ci-ty (2) gleams (1) = 5 ✓\\n\\n\\n\"Summer warmth embraced\" = Sum-mer (2) warmth (1) em-braced (2) = 5 ✓\\n\\nI\\'m struggling with the syllable count for the first line. I\\'ll keep experimenting to find a 5-syllable line that captures the sunny New York atmosphere.\\n\\n\"City glows warmly bright\" looks promising. I\\'ll carefully count each syllable to ensure accuracy. Breaking down \"City glows warmly bright\" reveals a potential solution, with each word contributing to the precise 5-syllable structure I need.\\n\\nI\\'ll continue exploring variations that might work within the haiku\\'s strict rhythmic constraints. The challenge is finding the right balance between imagery and syllable count.\\n\\nMy next step is to test different phrasings that could potentially fit the 5-7-5 syllable pattern, focusing on capturing the essence of a sunny day in New York while maintaining the precise syllabic structure.\\n\\nI\\'ll try a haiku that emphasizes the brightness and urban landscape, carefully counting syllables to ensure each line meets the traditional requirements. The challenge is to create imagery that feels both vivid and concise, highlighting the interplay between sunlight and cityscape.\\n\\nMy current draft explores how to distill the feeling of a bright urban day into a precise poetic form, paying close attention to the rhythmic and syllabic constraints of the haiku style.\\n\\nThe lines suggest a sensory experience of sunlight illuminating architectural elements, with a subtle progression from light to atmospheric conditions. Each word choice aims to capture a moment of urban luminosity.\\n\\nThe syllabic structure reveals precise attention to rhythmic constraints, carefully mapping each syllabic stress to maintain poetic form while conveying a vivid image of New York City\\'s atmospheric brightness.\\n\\nI\\'ll craft a concise response highlighting the urban luminescence, capturing the essence of a sunny day through a carefully structured haiku that reflects the tool\\'s concise, informative output.', type='summary_text')], type='reasoning', content=None, encrypted_content=None, status=None, provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'dc121fdb476a27dc6cfcdf1e808e6190'}),\n",
      "                                         ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='Sunny New York day\\nGolden light fills the streets\\nSummer warmth embraces', type='output_text', logprobs=[])], role='assistant', status='completed', type='message', provider_data={'model': 'huggingface/novita/MiniMaxAI/MiniMax-M2.1', 'response_id': 'dc121fdb476a27dc6cfcdf1e808e6190'})],\n",
      "                                 usage=Usage(requests=1,\n",
      "                                             input_tokens=231,\n",
      "                                             input_tokens_details=InputTokensDetails(cached_tokens=231),\n",
      "                                             output_tokens=829,\n",
      "                                             output_tokens_details=OutputTokensDetails(reasoning_tokens=0),\n",
      "                                             total_tokens=1060,\n",
      "                                             request_usage_entries=[]),\n",
      "                                 response_id=None)],\n",
      " 'tool_input_guardrail_results': [],\n",
      " 'tool_output_guardrail_results': []}\n"
     ]
    }
   ],
   "source": [
    "pprint(result.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c005cc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunny New York day\n",
      "Golden light fills the streets\n",
      "Summer warmth embraces\n"
     ]
    }
   ],
   "source": [
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8393c03",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccb06d",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "\n",
    "- Use the `output_type` param with Pydantic objects (or dataclasses, lists, TypedDicts, etc.)\n",
    "- When using non-openai models, remember to check that they support both Structured Ouput AND tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84cc7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n",
      "WARNING:openai.agents:[non-fatal] Tracing: server error 503, retrying.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Meeting with Alice and Bob' date='July 5th' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import Agent\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "model = LitellmModel(\n",
    "    model=\"huggingface/novita/zai-org/GLM-4.7\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Calendar extractor\",\n",
    "    instructions=\"Extract calendar events from text\",\n",
    "    output_type=CalendarEvent,\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    agent,\n",
    "    \"Extract the event from the following text: 'Meeting with Alice and Bob on July 5th.'\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846818ce",
   "metadata": {},
   "source": [
    "### Multi-Agent systems\n",
    "\n",
    "Two main architectures:\n",
    "- Manager (agents as tools): A central manager/orchestrator invokes specialized sub‑agents as tools and retains control of the conversation.\n",
    "- Handoffs: Peer agents hand off control to a specialized agent that takes over the conversation. This is decentralized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0549c",
   "metadata": {},
   "source": [
    "#### Agents Handoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4055da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Causes of World War II\n",
      "\n",
      "World War II (1939-1945) emerged from a complex combination of political, economic, and social factors. Here are the key causes:\n",
      "\n",
      "## 1. Treaty of Versailles (1919)\n",
      "The peace settlement ending World War I imposed harsh penalties on Germany, including:\n",
      "- Massive reparations payments\n",
      "- Territorial losses\n",
      "- Military restrictions\n",
      "- War guilt clause assigning full responsibility to Germany\n",
      "\n",
      "These provisions created deep resentment in Germany and economic hardship that fueled extremist politics.\n",
      "\n",
      "## 2. Rise of Totalitarian Regimes\n",
      "- **Germany:** Adolf Hitler and the Nazi Party came to power in 1933, promoting German nationalism, anti-Semitism, and territorial expansion.\n",
      "- **Italy:** Benito Mussolini established a fascist dictatorship in the 1920s with imperial ambitions.\n",
      "- **Japan:** Military leaders gained control, pursuing aggressive expansion in Asia.\n",
      "\n",
      "## 3. Expansionist Aggression\n",
      "- **Germany:** Remilitarized the Rhineland (1936), annexed Austria (1938), and demanded the Sudetenland from Czechoslovakia.\n",
      "- **Italy:** Invaded Ethiopia (1935) and Albania (1939).\n",
      "- **Japan:** Invaded Manchuria (1931) and China (1937).\n",
      "\n",
      "## 4. Policy of Appeasement\n",
      "Britain and France, hoping to avoid another war, made concessions to Hitler, most notably at the Munich Conference (1938), allowing Germany to take the Sudetenland. This failed to satisfy Hitler's ambitions.\n",
      "\n",
      "## 5. Failure of the League of Nations\n",
      "The League proved ineffective at preventing aggression due to:\n",
      "- Lack of enforcement power\n",
      "- Absence of key powers (including the U.S.)\n",
      "- Decision-making requiring unanimous agreement\n",
      "\n",
      "## 6. The Great Depression\n",
      "The worldwide economic crisis of the 1930s:\n",
      "- Destabilized democracies\n",
      "- Fueled extremist movements\n",
      "- Increased demand for resources and markets\n",
      "\n",
      "## 7. Nazi-Soviet Pact (1939)\n",
      "Surprisingly, Germany and the Soviet Union signed a non-aggression pact, allowing Hitler to invade Poland without facing a two-front war initially.\n",
      "\n",
      "The immediate trigger came on September 1, 1939, when Germany invaded Poland, leading Britain and France to declare war on Germany two days later.\n",
      "\n",
      "Would you like me to elaborate on any of these causes or discuss other aspects of World War II?\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent\n",
    "\n",
    "glm_model = LitellmModel(\n",
    "    model=\"huggingface/novita/zai-org/GLM-4.7\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    triage_agent,\n",
    "    \"Can you explain the causes of World War II?\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebdb44",
   "metadata": {},
   "source": [
    "#### Agents As Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d2e5b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The causes of World War II were complex and rooted in unresolved issues from World War I, economic instability, and the rise of aggressive totalitarian regimes. Here's a breakdown:\n",
      "\n",
      "## 1. The Treaty of Versailles (1919)\n",
      "- Imposed harsh penalties on Germany: accepted full responsibility, paid massive reparations, lost territory, faced military restrictions\n",
      "- Created deep resentment and humiliation, fueling radical political movements\n",
      "\n",
      "## 2. Rise of Totalitarianism and Fascism\n",
      "- **Adolf Hitler (Germany):** Promoted German superiority (*Lebensraum*) and sought to overturn the Treaty\n",
      "- **Benito Mussolini (Italy):** Pursued imperial ambitions in Africa and the Mediterranean\n",
      "- **Military Japan:** Sought domination of East Asia to secure resources\n",
      "\n",
      "## 3. Failure of the League of Nations\n",
      "- Lacked enforcement power and key member support\n",
      "- Weak response to Japan's invasion of Manchuria (1931) and Italy's invasion of Ethiopia (1935)\n",
      "- Emboldened aggressors by showing they could act without consequences\n",
      "\n",
      "## 4. The Great Depression (1929)\n",
      "- Global economic collapse destabilized democracies\n",
      "- Enabled radical parties to gain popularity by promising solutions\n",
      "\n",
      "## 5. Expansionism and Appeasement\n",
      "- Hitler violated the Treaty step by step (Rhineland 1936, Austria 1938, Sudetenland 1938)\n",
      "- Britain and France pursued **appeasement**—giving in to Hitler's demands to avoid war\n",
      "- The Munich Agreement (1938) allowed Germany to take the Sudetenland; Hitler's promise of peace was a lie\n",
      "\n",
      "## 6. The Nazi-Soviet Pact (August 1939)\n",
      "- Germany and the USSR signed a non-aggression treaty\n",
      "- Secretly agreed to divide Poland between them\n",
      "- Freed Hitler to invade Poland without fear of a two-front war\n",
      "\n",
      "## The Immediate Trigger: Invasion of Poland\n",
      "On **September 1, 1939**, Germany invaded Poland using Blitzkrieg tactics. Britain and France declared war on September 3, 1939, marking the official start of World War II in Europe.\n",
      "\n",
      "---\n",
      "\n",
      "In summary, while the invasion of Poland was the spark that ignited the war, deeper causes included resentment over WWI, economic desperation, international powerlessness, and unchecked fascist aggression.\n"
     ]
    }
   ],
   "source": [
    "manager_agent = Agent(\n",
    "    name=\"Manager Agent\",\n",
    "    instructions=\"You manage a team of agents to answer user questions effectively.\",\n",
    "    tools=[\n",
    "        history_tutor_agent.as_tool(\n",
    "            tool_name=\"history_tutor_agent\",\n",
    "            tool_description=\"Handles historical queries\",\n",
    "        ),\n",
    "        math_tutor_agent.as_tool(\n",
    "            tool_name=\"math_tutor_agent\",\n",
    "            tool_description=\"Handles math questions\",\n",
    "        ),\n",
    "    ],\n",
    "    model=glm_model,\n",
    "    model_settings=ModelSettings(include_usage=True),\n",
    ")\n",
    "\n",
    "result = await Runner.run(\n",
    "    manager_agent,\n",
    "    \"Can you explain the causes of World War II?\",\n",
    ")\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ac28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
